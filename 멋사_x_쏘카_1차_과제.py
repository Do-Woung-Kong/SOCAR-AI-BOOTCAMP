# -*- coding: utf-8 -*-
"""멋사 X 쏘카 1차 과제

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nJ1tGHVLL3L8fEo_YjfPC1vqCFBQd-w1

# 1.데이터 로드 및 모듈 호출
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

tada_eta = pd.read_excel('/content/drive/MyDrive/<멋쟁이사자처럼> 쏘카/Chapter_2/Chapter 2_실습파일/tada_eta.xlsx')
print(tada_eta.columns)
tada_eta.head()

tada_eta['distance'] = ((tada_eta['pickup_lat']-tada_eta['driver_lat'])**2 + (tada_eta['pickup_lng']-tada_eta['driver_lng'])**2)*100000
tada_eta = tada_eta.drop(['id', 'created_at_kst', 'driver_id', 'pickup_lng', 'pickup_lat', 'driver_lng','driver_lat'],1)
tada_eta.head()

tada_eta = pd.get_dummies(tada_eta)
tada_eta.head()

"""# 2.데이터셋 분리"""

tada_eta = tada_eta.sample(frac=1, random_state=0).reset_index(drop=True)
train = tada_eta[:12000]
test = tada_eta[12000:]

x_test = test.drop('ATA',1)
y_test = test['ATA']

"""# 3.이상치 제거"""

from collections import Counter 

def detect_outliers(df, n, features): 
  outlier_indices = [] 
  for col in features:
    Q1 = np.percentile(df[col], 5) 
    Q3 = np.percentile(df[col], 95) 
    IQR = Q3 - Q1 
    outlier_step = 1.5 * IQR 
    outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index 
    outlier_indices.extend(outlier_list_col) 
  outlier_indices = Counter(outlier_indices) 
  multiple_outliers = list(k for k, v in outlier_indices.items() if v > n) 

  return multiple_outliers 
  
Outliers_to_drop = detect_outliers(train, 0, ['ATA', 'api_eta', 'month', 'hour', 'distance'])

train.loc[Outliers_to_drop]

pre_train = train.drop(Outliers_to_drop, axis=0).reset_index(drop=True)

x_train = pre_train.drop('ATA',1)
y_train = pre_train['ATA']

"""# 4.RFECV를 통한 변수선택"""

from sklearn.feature_selection import RFECV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_jobs=10,random_state=42)
rfe = RFECV(model,cv=5)
rfe = rfe.fit(x_train, y_train)

print('Selected features; %s' % list(x_train.columns[rfe.support_]))

len(list(x_train.columns[rfe.support_]))

x_train_pre = x_train[['api_eta', 'month', 'hour', 'distance', 'pickup_gu_강남구', 'pickup_gu_강서구', 'pickup_gu_관악구', 'pickup_gu_광진구', 'pickup_gu_구로구', 'pickup_gu_금천구', 'pickup_gu_동대문구', 'pickup_gu_동작구', 'pickup_gu_마포구', 'pickup_gu_서대문구', 'pickup_gu_서초구', 'pickup_gu_성동구', 'pickup_gu_성북구', 'pickup_gu_송파구', 'pickup_gu_양천구', 'pickup_gu_영등포구', 'pickup_gu_용산구', 'pickup_gu_은평구', 'pickup_gu_종로구', 'pickup_gu_중구']]

"""# 5.OPTUNA를 활용한 하이퍼파라미터 튜닝"""

!pip install optuna

x_test_pre = x_test[['api_eta', 'month', 'hour', 'distance', 'pickup_gu_강남구', 'pickup_gu_강서구', 'pickup_gu_관악구', 'pickup_gu_광진구', 'pickup_gu_구로구', 'pickup_gu_금천구', 'pickup_gu_동대문구', 'pickup_gu_동작구', 'pickup_gu_마포구', 'pickup_gu_서대문구', 'pickup_gu_서초구', 'pickup_gu_성동구', 'pickup_gu_성북구', 'pickup_gu_송파구', 'pickup_gu_양천구', 'pickup_gu_영등포구', 'pickup_gu_용산구', 'pickup_gu_은평구', 'pickup_gu_종로구', 'pickup_gu_중구']]

X = np.concatenate((x_train_pre,x_test_pre), axis=0)
y = np.concatenate((y_train,y_test), axis=0)

import optuna
from optuna import Trial, visualization
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import PredefinedSplit
from sklearn import ensemble

def objective(trial):
    max_depth = trial.suggest_int('max_depth', 1, 10000)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-8, 1e-2)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10000)
    n_estimators =  trial.suggest_int('n_estimators', 30, 1000)
    loss = trial.suggest_categorical('loss', ['ls'])

    
    regr = ensemble.GradientBoostingRegressor(max_depth = max_depth, learning_rate = learning_rate,
                                 min_samples_leaf = min_samples_leaf,n_estimators = n_estimators,loss=loss)
    
    pds = PredefinedSplit(test_fold=[-1]*len(x_train_pre)+[0]*len(x_test_pre))
    score = cross_val_score(regr,X, y, cv=pds, scoring="neg_mean_squared_error")
    neg_mean_squared_error_mean = score.mean()

    return neg_mean_squared_error_mean

#Execute optuna and set hyperparameters
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

#Create an instance with tuned hyperparameters
optimised_Gbr = ensemble.GradientBoostingRegressor(max_depth = study.best_params['max_depth'], learning_rate = study.best_params['learning_rate'],
                                     min_samples_leaf = study.best_params['min_samples_leaf'],n_estimators = study.best_params['n_estimators'],loss = study.best_params['loss'],random_state=42)

"""# 최종 MSE(8.2084)"""

optimised_try = ensemble.GradientBoostingRegressor(max_depth = 121, learning_rate = 0.008709674733674474,
                                     min_samples_leaf = 361,n_estimators = 960,loss = 'ls',random_state=42)

optimised_try = ensemble.GradientBoostingRegressor(max_depth = 2179, learning_rate = 0.009713819638907639,
                                     min_samples_leaf = 334,n_estimators = 484,loss = 'ls',random_state=42)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

optimised_try.fit(x_train_pre,y_train)

mse = mean_squared_error(y_test, optimised_try.predict(x_test_pre))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse))
#print("The initial error of API ETA on test set: {:.4f}".format(mean_squared_error(y_test, x_test[:,0]) ))

mae = mean_absolute_error(y_test, optimised_try.predict(x_test_pre))
print("The mean absolute error (MAE) on test set: {:.4f}".format(mae))
#print("The initial error of API ETA on test set: {:.4f}".format(mean_absolute_error(y_test, x_test[:,0]) ))